{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from ppo_multi.history import *\n",
    "from ppo_multi.models import *\n",
    "from ppo_multi.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo-hunter13\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model =  True # Whether to train the model.\n",
    "summary_freq = 1000 # Frequency at which to save training statistics.\n",
    "save_freq = 5000 # Frequency at which to save model.\n",
    "env_name = \"hunter\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.997 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 10240 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 5e-5 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 1024 # How many experiences per gradient descent update step.\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents.environment:\n",
      "'HunterAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: HunterAcademy\n",
      "        Number of brains: 2\n",
      "        Reset Parameters :\n",
      "\t\thunterSpeed -> 0.1\n",
      "\t\tringRadius -> 20.0\n",
      "\t\thunteeSpeed -> 0.15\n",
      "\t\tsphereRadius -> 2.0\n",
      "Unity brain name: HunteeBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 6\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n",
      "Unity brain name: HunterBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 6\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file, worker_id = 2)\n",
    "print(str(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 356000. Mean Reward: 0.497947019868. Std of Reward: 0.712351128057.\n",
      "Brain : HunterBrain. Step: 356000. Mean Reward: 0.0417762592641. Std of Reward: 0.401382803244.\n",
      "Brain : HunteeBrain. Step: 357000. Mean Reward: 0.396325301205. Std of Reward: 0.756975825376.\n",
      "Brain : HunterBrain. Step: 357000. Mean Reward: 0.0762353173678. Std of Reward: 0.433021941047.\n",
      "Brain : HunteeBrain. Step: 358000. Mean Reward: 0.45987804878. Std of Reward: 0.741301580943.\n",
      "Brain : HunterBrain. Step: 358000. Mean Reward: 0.028860659191. Std of Reward: 0.39179231734.\n",
      "Brain : HunteeBrain. Step: 359000. Mean Reward: 0.353964497041. Std of Reward: 0.767354032867.\n",
      "Brain : HunterBrain. Step: 359000. Mean Reward: 0.0594347695723. Std of Reward: 0.425898706917.\n",
      "Brain : HunteeBrain. Step: 360000. Mean Reward: 0.345380116959. Std of Reward: 0.765549339483.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 360000. Mean Reward: -0.00399866081727. Std of Reward: 0.369251863612.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 361000. Mean Reward: 0.375. Std of Reward: 0.759615271994.\n",
      "Brain : HunterBrain. Step: 361000. Mean Reward: 0.0554963243133. Std of Reward: 0.421398703331.\n",
      "Brain : HunteeBrain. Step: 362000. Mean Reward: 0.424382716049. Std of Reward: 0.743778260264.\n",
      "Brain : HunterBrain. Step: 362000. Mean Reward: 0.103637382726. Std of Reward: 0.449344324081.\n",
      "Brain : HunteeBrain. Step: 363000. Mean Reward: 0.387012195122. Std of Reward: 0.745134772454.\n",
      "Brain : HunterBrain. Step: 363000. Mean Reward: 0.0201113035724. Std of Reward: 0.388637901839.\n",
      "Brain : HunteeBrain. Step: 364000. Mean Reward: 0.227540983607. Std of Reward: 0.803694316473.\n",
      "Brain : HunterBrain. Step: 364000. Mean Reward: 0.141627934439. Std of Reward: 0.481450199435.\n",
      "Brain : HunteeBrain. Step: 365000. Mean Reward: 0.432160493827. Std of Reward: 0.744590963306.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 365000. Mean Reward: 0.0699720753453. Std of Reward: 0.433345952062.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 366000. Mean Reward: 0.308235294118. Std of Reward: 0.775211055088.\n",
      "Brain : HunterBrain. Step: 366000. Mean Reward: 0.0177496316178. Std of Reward: 0.387424950489.\n",
      "Brain : HunteeBrain. Step: 367000. Mean Reward: 0.292967032967. Std of Reward: 0.80496193959.\n",
      "Brain : HunterBrain. Step: 367000. Mean Reward: 0.0993014050393. Std of Reward: 0.456999417486.\n",
      "Brain : HunteeBrain. Step: 368000. Mean Reward: 0.350963855422. Std of Reward: 0.761442218232.\n",
      "Brain : HunterBrain. Step: 368000. Mean Reward: 0.0424737062177. Std of Reward: 0.414967908664.\n",
      "Brain : HunteeBrain. Step: 369000. Mean Reward: 0.301411764706. Std of Reward: 0.774369805168.\n",
      "Brain : HunterBrain. Step: 369000. Mean Reward: 0.0589708492139. Std of Reward: 0.424006443956.\n",
      "Brain : HunteeBrain. Step: 370000. Mean Reward: 0.341395348837. Std of Reward: 0.766650314136.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 370000. Mean Reward: 0.113001214701. Std of Reward: 0.456979052709.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 371000. Mean Reward: 0.31216374269. Std of Reward: 0.784077285453.\n",
      "Brain : HunterBrain. Step: 371000. Mean Reward: 0.0456632094403. Std of Reward: 0.416031325503.\n",
      "Brain : HunteeBrain. Step: 372000. Mean Reward: 0.327705882353. Std of Reward: 0.765043656171.\n",
      "Brain : HunterBrain. Step: 372000. Mean Reward: 0.0759285714659. Std of Reward: 0.439170518608.\n",
      "Brain : HunteeBrain. Step: 373000. Mean Reward: 0.410958083832. Std of Reward: 0.752414264742.\n",
      "Brain : HunterBrain. Step: 373000. Mean Reward: 0.0949009172045. Std of Reward: 0.440305715077.\n",
      "Brain : HunteeBrain. Step: 374000. Mean Reward: 0.358928571429. Std of Reward: 0.772782569459.\n",
      "Brain : HunterBrain. Step: 374000. Mean Reward: 0.0800692906471. Std of Reward: 0.434546266521.\n",
      "Brain : HunteeBrain. Step: 375000. Mean Reward: 0.372916666667. Std of Reward: 0.768300036077.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 375000. Mean Reward: 0.0292989553545. Std of Reward: 0.390981568817.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 376000. Mean Reward: 0.307458563536. Std of Reward: 0.808716433852.\n",
      "Brain : HunterBrain. Step: 376000. Mean Reward: 0.143684097799. Std of Reward: 0.477419519522.\n",
      "Brain : HunteeBrain. Step: 377000. Mean Reward: 0.3175. Std of Reward: 0.777194090792.\n",
      "Brain : HunterBrain. Step: 377000. Mean Reward: 0.0802018918701. Std of Reward: 0.433478522533.\n",
      "Brain : HunteeBrain. Step: 378000. Mean Reward: 0.235138121547. Std of Reward: 0.801914565196.\n",
      "Brain : HunterBrain. Step: 378000. Mean Reward: 0.117785569373. Std of Reward: 0.458807642199.\n",
      "Brain : HunteeBrain. Step: 379000. Mean Reward: 0.306149425287. Std of Reward: 0.78062206301.\n",
      "Brain : HunterBrain. Step: 379000. Mean Reward: 0.107763939078. Std of Reward: 0.459984686358.\n",
      "Brain : HunteeBrain. Step: 380000. Mean Reward: 0.398571428571. Std of Reward: 0.73980260683.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 380000. Mean Reward: 0.014719805813. Std of Reward: 0.383783812108.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 381000. Mean Reward: 0.366315789474. Std of Reward: 0.767141649002.\n",
      "Brain : HunterBrain. Step: 381000. Mean Reward: 0.11752140718. Std of Reward: 0.460236278459.\n",
      "Brain : HunteeBrain. Step: 382000. Mean Reward: 0.278208092486. Std of Reward: 0.780348019695.\n",
      "Brain : HunterBrain. Step: 382000. Mean Reward: 0.108810566758. Std of Reward: 0.457586436219.\n",
      "Brain : HunteeBrain. Step: 383000. Mean Reward: 0.32132183908. Std of Reward: 0.796712339299.\n",
      "Brain : HunterBrain. Step: 383000. Mean Reward: 0.106651992964. Std of Reward: 0.452948332633.\n",
      "Brain : HunteeBrain. Step: 384000. Mean Reward: 0.390060240964. Std of Reward: 0.757551936259.\n",
      "Brain : HunterBrain. Step: 384000. Mean Reward: 0.0865820301506. Std of Reward: 0.436595583276.\n",
      "Brain : HunteeBrain. Step: 385000. Mean Reward: 0.320287356322. Std of Reward: 0.792750506592.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 385000. Mean Reward: 0.112209238835. Std of Reward: 0.454911381239.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 386000. Mean Reward: 0.289606741573. Std of Reward: 0.796060075058.\n",
      "Brain : HunterBrain. Step: 386000. Mean Reward: 0.0977355910597. Std of Reward: 0.447676974251.\n",
      "Brain : HunteeBrain. Step: 387000. Mean Reward: 0.249096045198. Std of Reward: 0.800140719853.\n",
      "Brain : HunterBrain. Step: 387000. Mean Reward: 0.13052191893. Std of Reward: 0.467375168841.\n",
      "Brain : HunteeBrain. Step: 388000. Mean Reward: 0.296457142857. Std of Reward: 0.785870957886.\n",
      "Brain : HunterBrain. Step: 388000. Mean Reward: 0.0822389341502. Std of Reward: 0.441155569693.\n",
      "Brain : HunteeBrain. Step: 389000. Mean Reward: 0.191720430108. Std of Reward: 0.811418674869.\n",
      "Brain : HunterBrain. Step: 389000. Mean Reward: 0.168320127121. Std of Reward: 0.49468499256.\n",
      "Brain : HunteeBrain. Step: 390000. Mean Reward: 0.224108108108. Std of Reward: 0.811178248275.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 390000. Mean Reward: 0.124955979808. Std of Reward: 0.467508371079.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 391000. Mean Reward: 0.249005524862. Std of Reward: 0.811076022877.\n",
      "Brain : HunterBrain. Step: 391000. Mean Reward: 0.103051154618. Std of Reward: 0.455202034065.\n",
      "Brain : HunteeBrain. Step: 392000. Mean Reward: 0.161458333333. Std of Reward: 0.821138030376.\n",
      "Brain : HunterBrain. Step: 392000. Mean Reward: 0.139382151318. Std of Reward: 0.479154344476.\n",
      "Brain : HunteeBrain. Step: 393000. Mean Reward: 0.278612716763. Std of Reward: 0.796261739333.\n",
      "Brain : HunterBrain. Step: 393000. Mean Reward: 0.161186215041. Std of Reward: 0.487295101466.\n",
      "Brain : HunteeBrain. Step: 394000. Mean Reward: 0.223101604278. Std of Reward: 0.818211173497.\n",
      "Brain : HunterBrain. Step: 394000. Mean Reward: 0.115974204334. Std of Reward: 0.465669949455.\n",
      "Brain : HunteeBrain. Step: 395000. Mean Reward: 0.325406976744. Std of Reward: 0.778263838176.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 395000. Mean Reward: 0.0786769084459. Std of Reward: 0.430991369873.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 396000. Mean Reward: 0.221857923497. Std of Reward: 0.806690494294.\n",
      "Brain : HunterBrain. Step: 396000. Mean Reward: 0.127148990274. Std of Reward: 0.469526818003.\n",
      "Brain : HunteeBrain. Step: 397000. Mean Reward: 0.175. Std of Reward: 0.82279545965.\n",
      "Brain : HunterBrain. Step: 397000. Mean Reward: 0.122105356739. Std of Reward: 0.464287139163.\n",
      "Brain : HunteeBrain. Step: 398000. Mean Reward: 0.203681318681. Std of Reward: 0.80091511368.\n",
      "Brain : HunterBrain. Step: 398000. Mean Reward: 0.0558650903383. Std of Reward: 0.418411859337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain : HunteeBrain. Step: 399000. Mean Reward: 0.184139784946. Std of Reward: 0.817327510873.\n",
      "Brain : HunterBrain. Step: 399000. Mean Reward: 0.141110768333. Std of Reward: 0.477831141802.\n",
      "Brain : HunteeBrain. Step: 400000. Mean Reward: 0.212043010753. Std of Reward: 0.817636324683.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 400000. Mean Reward: 0.0988611592677. Std of Reward: 0.447513207149.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 401000. Mean Reward: 0.1615625. Std of Reward: 0.823448450275.\n",
      "Brain : HunterBrain. Step: 401000. Mean Reward: 0.198772913329. Std of Reward: 0.494851075607.\n",
      "Brain : HunteeBrain. Step: 402000. Mean Reward: 0.249450549451. Std of Reward: 0.821705359666.\n",
      "Brain : HunterBrain. Step: 402000. Mean Reward: 0.138545031366. Std of Reward: 0.473024148064.\n",
      "Brain : HunteeBrain. Step: 403000. Mean Reward: 0.348779069767. Std of Reward: 0.774636612498.\n",
      "Brain : HunterBrain. Step: 403000. Mean Reward: 0.0824715727771. Std of Reward: 0.434559299813.\n",
      "Brain : HunteeBrain. Step: 404000. Mean Reward: 0.202540540541. Std of Reward: 0.811783841372.\n",
      "Brain : HunterBrain. Step: 404000. Mean Reward: 0.166449470998. Std of Reward: 0.486226261665.\n",
      "Brain : HunteeBrain. Step: 405000. Mean Reward: 0.258926553672. Std of Reward: 0.803321129072.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 405000. Mean Reward: 0.152111873662. Std of Reward: 0.476859734303.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 406000. Mean Reward: 0.205614973262. Std of Reward: 0.81440770866.\n",
      "Brain : HunterBrain. Step: 406000. Mean Reward: 0.0983330259896. Std of Reward: 0.450803857934.\n",
      "Brain : HunteeBrain. Step: 407000. Mean Reward: 0.169105263158. Std of Reward: 0.807906156271.\n",
      "Brain : HunterBrain. Step: 407000. Mean Reward: 0.074394466457. Std of Reward: 0.439278016588.\n",
      "Brain : HunteeBrain. Step: 408000. Mean Reward: 0.146875. Std of Reward: 0.829176062551.\n",
      "Brain : HunterBrain. Step: 408000. Mean Reward: 0.152846572584. Std of Reward: 0.485415882715.\n",
      "Brain : HunteeBrain. Step: 409000. Mean Reward: 0.22489010989. Std of Reward: 0.811219228246.\n",
      "Brain : HunterBrain. Step: 409000. Mean Reward: 0.109019635322. Std of Reward: 0.46099155254.\n",
      "Brain : HunteeBrain. Step: 410000. Mean Reward: 0.146666666667. Std of Reward: 0.803714919581.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 410000. Mean Reward: 0.0929656447172. Std of Reward: 0.447579114956.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 411000. Mean Reward: 0.140157894737. Std of Reward: 0.820497587357.\n",
      "Brain : HunterBrain. Step: 411000. Mean Reward: 0.155422705132. Std of Reward: 0.484992386527.\n",
      "Brain : HunteeBrain. Step: 412000. Mean Reward: 0.204810810811. Std of Reward: 0.80777043293.\n",
      "Brain : HunterBrain. Step: 412000. Mean Reward: 0.118582102374. Std of Reward: 0.457763937123.\n",
      "Brain : HunteeBrain. Step: 413000. Mean Reward: 0.274033149171. Std of Reward: 0.808096309427.\n",
      "Brain : HunterBrain. Step: 413000. Mean Reward: 0.133641459845. Std of Reward: 0.469686522471.\n",
      "Brain : HunteeBrain. Step: 414000. Mean Reward: 0.335119047619. Std of Reward: 0.782821505605.\n",
      "Brain : HunterBrain. Step: 414000. Mean Reward: 0.116844488457. Std of Reward: 0.455866758947.\n",
      "Brain : HunteeBrain. Step: 415000. Mean Reward: 0.228043478261. Std of Reward: 0.808177954912.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 415000. Mean Reward: 0.148665371018. Std of Reward: 0.480501730309.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 416000. Mean Reward: 0.247666666667. Std of Reward: 0.794857149989.\n",
      "Brain : HunterBrain. Step: 416000. Mean Reward: 0.063485491224. Std of Reward: 0.424440490817.\n",
      "Brain : HunteeBrain. Step: 417000. Mean Reward: 0.24. Std of Reward: 0.792141085918.\n",
      "Brain : HunterBrain. Step: 417000. Mean Reward: 0.094900190042. Std of Reward: 0.446031145748.\n",
      "Brain : HunteeBrain. Step: 418000. Mean Reward: 0.229065934066. Std of Reward: 0.806919053121.\n",
      "Brain : HunterBrain. Step: 418000. Mean Reward: 0.107598637954. Std of Reward: 0.458103989149.\n",
      "Brain : HunteeBrain. Step: 419000. Mean Reward: 0.276333333333. Std of Reward: 0.809422221917.\n",
      "Brain : HunterBrain. Step: 419000. Mean Reward: 0.164529175603. Std of Reward: 0.486958814628.\n",
      "Brain : HunteeBrain. Step: 420000. Mean Reward: 0.38125. Std of Reward: 0.770683206101.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 420000. Mean Reward: 0.0308203371136. Std of Reward: 0.391299143382.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 421000. Mean Reward: 0.358. Std of Reward: 0.773800855974.\n",
      "Brain : HunterBrain. Step: 421000. Mean Reward: 0.0534040039343. Std of Reward: 0.419626291714.\n",
      "Brain : HunteeBrain. Step: 422000. Mean Reward: 0.358520710059. Std of Reward: 0.770229083641.\n",
      "Brain : HunterBrain. Step: 422000. Mean Reward: 0.0437266985492. Std of Reward: 0.402412856862.\n",
      "Brain : HunteeBrain. Step: 423000. Mean Reward: 0.201216931217. Std of Reward: 0.817210444508.\n",
      "Brain : HunterBrain. Step: 423000. Mean Reward: 0.0905149031769. Std of Reward: 0.44908157212.\n",
      "Brain : HunteeBrain. Step: 424000. Mean Reward: 0.224054054054. Std of Reward: 0.825820016578.\n",
      "Brain : HunterBrain. Step: 424000. Mean Reward: 0.135134048141. Std of Reward: 0.472147037603.\n",
      "Brain : HunteeBrain. Step: 425000. Mean Reward: 0.185026737968. Std of Reward: 0.81588269468.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 425000. Mean Reward: 0.14921477778. Std of Reward: 0.478101853911.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 426000. Mean Reward: 0.281511627907. Std of Reward: 0.781330882238.\n",
      "Brain : HunterBrain. Step: 426000. Mean Reward: 0.104232168123. Std of Reward: 0.450142886292.\n",
      "Brain : HunteeBrain. Step: 427000. Mean Reward: 0.235376344086. Std of Reward: 0.824561870159.\n",
      "Brain : HunterBrain. Step: 427000. Mean Reward: 0.170602918012. Std of Reward: 0.486521743638.\n",
      "Brain : HunteeBrain. Step: 428000. Mean Reward: 0.201657754011. Std of Reward: 0.81793461716.\n",
      "Brain : HunterBrain. Step: 428000. Mean Reward: 0.129772519973. Std of Reward: 0.470271833188.\n",
      "Brain : HunteeBrain. Step: 429000. Mean Reward: 0.3212. Std of Reward: 0.798980950962.\n",
      "Brain : HunterBrain. Step: 429000. Mean Reward: 0.0809282876814. Std of Reward: 0.43547701201.\n",
      "Brain : HunteeBrain. Step: 430000. Mean Reward: 0.374345238095. Std of Reward: 0.77232536929.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 430000. Mean Reward: 0.056822029394. Std of Reward: 0.412553599307.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 431000. Mean Reward: 0.263463687151. Std of Reward: 0.807762107915.\n",
      "Brain : HunterBrain. Step: 431000. Mean Reward: 0.106468943794. Std of Reward: 0.455561170775.\n",
      "Brain : HunteeBrain. Step: 432000. Mean Reward: 0.289265536723. Std of Reward: 0.78864972397.\n",
      "Brain : HunterBrain. Step: 432000. Mean Reward: 0.0840814471173. Std of Reward: 0.443017857496.\n",
      "Brain : HunteeBrain. Step: 433000. Mean Reward: 0.273798882682. Std of Reward: 0.803737139691.\n",
      "Brain : HunterBrain. Step: 433000. Mean Reward: 0.110046957834. Std of Reward: 0.46186738896.\n",
      "Brain : HunteeBrain. Step: 434000. Mean Reward: 0.21. Std of Reward: 0.827046846609.\n",
      "Brain : HunterBrain. Step: 434000. Mean Reward: 0.174426450204. Std of Reward: 0.493212376411.\n",
      "Brain : HunteeBrain. Step: 435000. Mean Reward: 0.313351955307. Std of Reward: 0.802710453189.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 435000. Mean Reward: 0.0674864719775. Std of Reward: 0.42640571004.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 436000. Mean Reward: 0.182747252747. Std of Reward: 0.815083176412.\n",
      "Brain : HunterBrain. Step: 436000. Mean Reward: 0.104526210626. Std of Reward: 0.456988433512.\n",
      "Brain : HunteeBrain. Step: 437000. Mean Reward: 0.265621621622. Std of Reward: 0.814693951867.\n",
      "Brain : HunterBrain. Step: 437000. Mean Reward: 0.104231947093. Std of Reward: 0.447703387053.\n",
      "Brain : HunteeBrain. Step: 438000. Mean Reward: 0.174690721649. Std of Reward: 0.840592670898.\n",
      "Brain : HunterBrain. Step: 438000. Mean Reward: 0.22687165464. Std of Reward: 0.515533238236.\n",
      "Brain : HunteeBrain. Step: 439000. Mean Reward: 0.336. Std of Reward: 0.785506505758.\n",
      "Brain : HunterBrain. Step: 439000. Mean Reward: 0.101545965111. Std of Reward: 0.444264107628.\n",
      "Brain : HunteeBrain. Step: 440000. Mean Reward: 0.286983240223. Std of Reward: 0.810261599765.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 440000. Mean Reward: 0.0693492173722. Std of Reward: 0.433268703836.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 441000. Mean Reward: 0.320903954802. Std of Reward: 0.798388402736.\n",
      "Brain : HunterBrain. Step: 441000. Mean Reward: 0.123818588997. Std of Reward: 0.461348109794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain : HunteeBrain. Step: 442000. Mean Reward: 0.407469879518. Std of Reward: 0.767628711979.\n",
      "Brain : HunterBrain. Step: 442000. Mean Reward: 0.0741918242941. Std of Reward: 0.429181214875.\n",
      "Brain : HunteeBrain. Step: 443000. Mean Reward: 0.444397590361. Std of Reward: 0.737355620987.\n",
      "Brain : HunterBrain. Step: 443000. Mean Reward: 0.0736255252455. Std of Reward: 0.432352731415.\n",
      "Brain : HunteeBrain. Step: 444000. Mean Reward: 0.345941176471. Std of Reward: 0.790313785083.\n",
      "Brain : HunterBrain. Step: 444000. Mean Reward: 0.104331216611. Std of Reward: 0.451442055361.\n",
      "Brain : HunteeBrain. Step: 445000. Mean Reward: 0.203031914894. Std of Reward: 0.829454357332.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 445000. Mean Reward: 0.136348454197. Std of Reward: 0.480524328617.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 446000. Mean Reward: 0.217978723404. Std of Reward: 0.829147960242.\n",
      "Brain : HunterBrain. Step: 446000. Mean Reward: 0.16445971593. Std of Reward: 0.49211619186.\n",
      "Brain : HunteeBrain. Step: 447000. Mean Reward: 0.372941176471. Std of Reward: 0.780680432518.\n",
      "Brain : HunterBrain. Step: 447000. Mean Reward: 0.0485865361915. Std of Reward: 0.414058968968.\n",
      "Brain : HunteeBrain. Step: 448000. Mean Reward: 0.256576086957. Std of Reward: 0.819817526538.\n",
      "Brain : HunterBrain. Step: 448000. Mean Reward: 0.138850616251. Std of Reward: 0.475024208025.\n",
      "Brain : HunteeBrain. Step: 449000. Mean Reward: 0.163455497382. Std of Reward: 0.825176228589.\n",
      "Brain : HunterBrain. Step: 449000. Mean Reward: 0.106062854317. Std of Reward: 0.461439155297.\n",
      "Brain : HunteeBrain. Step: 450000. Mean Reward: 0.315574712644. Std of Reward: 0.795236871127.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 450000. Mean Reward: 0.111932398621. Std of Reward: 0.458653548644.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 451000. Mean Reward: 0.268418079096. Std of Reward: 0.803051233009.\n",
      "Brain : HunterBrain. Step: 451000. Mean Reward: 0.0763239396682. Std of Reward: 0.437296211905.\n",
      "Brain : HunteeBrain. Step: 452000. Mean Reward: 0.248043478261. Std of Reward: 0.816521305042.\n",
      "Brain : HunterBrain. Step: 452000. Mean Reward: 0.128289062368. Std of Reward: 0.475071293538.\n",
      "Brain : HunteeBrain. Step: 453000. Mean Reward: 0.190432432432. Std of Reward: 0.809234424344.\n",
      "Brain : HunterBrain. Step: 453000. Mean Reward: 0.0744862348835. Std of Reward: 0.437584955077.\n",
      "Brain : HunteeBrain. Step: 454000. Mean Reward: 0.336395348837. Std of Reward: 0.796101566488.\n",
      "Brain : HunterBrain. Step: 454000. Mean Reward: 0.0788476760953. Std of Reward: 0.434614707051.\n",
      "Brain : HunteeBrain. Step: 455000. Mean Reward: 0.40875. Std of Reward: 0.752223835972.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 455000. Mean Reward: 0.00309043293117. Std of Reward: 0.367406367698.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 456000. Mean Reward: 0.279944751381. Std of Reward: 0.810087299871.\n",
      "Brain : HunterBrain. Step: 456000. Mean Reward: 0.0970875925808. Std of Reward: 0.448051741448.\n",
      "Brain : HunteeBrain. Step: 457000. Mean Reward: 0.225762711864. Std of Reward: 0.804792572616.\n",
      "Brain : HunterBrain. Step: 457000. Mean Reward: 0.120662534377. Std of Reward: 0.467188907542.\n",
      "Brain : HunteeBrain. Step: 458000. Mean Reward: 0.192315789474. Std of Reward: 0.828383908751.\n",
      "Brain : HunterBrain. Step: 458000. Mean Reward: 0.0897582289658. Std of Reward: 0.443559388636.\n",
      "Brain : HunteeBrain. Step: 459000. Mean Reward: 0.28693989071. Std of Reward: 0.82044132579.\n",
      "Brain : HunterBrain. Step: 459000. Mean Reward: 0.142324681773. Std of Reward: 0.469599420974.\n",
      "Brain : HunteeBrain. Step: 460000. Mean Reward: 0.224838709677. Std of Reward: 0.82123512134.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 460000. Mean Reward: 0.105995756922. Std of Reward: 0.456384438047.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 461000. Mean Reward: 0.189732620321. Std of Reward: 0.817696276887.\n",
      "Brain : HunterBrain. Step: 461000. Mean Reward: 0.155133234642. Std of Reward: 0.484426588045.\n",
      "Brain : HunteeBrain. Step: 462000. Mean Reward: 0.197956989247. Std of Reward: 0.821117857426.\n",
      "Brain : HunterBrain. Step: 462000. Mean Reward: 0.121665285904. Std of Reward: 0.464457366775.\n",
      "Brain : HunteeBrain. Step: 463000. Mean Reward: 0.268296703297. Std of Reward: 0.808749448352.\n",
      "Brain : HunterBrain. Step: 463000. Mean Reward: 0.0791036248649. Std of Reward: 0.441682365874.\n",
      "Brain : HunteeBrain. Step: 464000. Mean Reward: 0.165706806283. Std of Reward: 0.836241527966.\n",
      "Brain : HunterBrain. Step: 464000. Mean Reward: 0.154383956098. Std of Reward: 0.485041231942.\n",
      "Brain : HunteeBrain. Step: 465000. Mean Reward: 0.309887640449. Std of Reward: 0.811685712108.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 465000. Mean Reward: 0.120696393165. Std of Reward: 0.462175004103.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 466000. Mean Reward: 0.220695187166. Std of Reward: 0.820318868567.\n",
      "Brain : HunterBrain. Step: 466000. Mean Reward: 0.105013331216. Std of Reward: 0.45761272394.\n",
      "Brain : HunteeBrain. Step: 467000. Mean Reward: 0.191397849462. Std of Reward: 0.825967131265.\n",
      "Brain : HunterBrain. Step: 467000. Mean Reward: 0.126871084826. Std of Reward: 0.476595764627.\n",
      "Brain : HunteeBrain. Step: 468000. Mean Reward: 0.185947368421. Std of Reward: 0.82883742128.\n",
      "Brain : HunterBrain. Step: 468000. Mean Reward: 0.109239932482. Std of Reward: 0.459646424068.\n",
      "Brain : HunteeBrain. Step: 469000. Mean Reward: 0.17921875. Std of Reward: 0.821718799518.\n",
      "Brain : HunterBrain. Step: 469000. Mean Reward: 0.184297122732. Std of Reward: 0.492431892204.\n",
      "Brain : HunteeBrain. Step: 470000. Mean Reward: 0.148826530612. Std of Reward: 0.84191481858.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 470000. Mean Reward: 0.177816636002. Std of Reward: 0.493421815215.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 471000. Mean Reward: 0.0685714285714. Std of Reward: 0.839445294545.\n",
      "Brain : HunterBrain. Step: 471000. Mean Reward: 0.158486751325. Std of Reward: 0.490896409552.\n",
      "Brain : HunteeBrain. Step: 472000. Mean Reward: 0.15335106383. Std of Reward: 0.818467005112.\n",
      "Brain : HunterBrain. Step: 472000. Mean Reward: 0.14207646308. Std of Reward: 0.481878403344.\n",
      "Brain : HunteeBrain. Step: 473000. Mean Reward: 0.215315789474. Std of Reward: 0.822619471575.\n",
      "Brain : HunterBrain. Step: 473000. Mean Reward: 0.132611536196. Std of Reward: 0.473187060042.\n",
      "Brain : HunteeBrain. Step: 474000. Mean Reward: 0.178697916667. Std of Reward: 0.835735624313.\n",
      "Brain : HunterBrain. Step: 474000. Mean Reward: 0.183574540254. Std of Reward: 0.493954386864.\n",
      "Brain : HunteeBrain. Step: 475000. Mean Reward: 0.414879518072. Std of Reward: 0.766085133172.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 475000. Mean Reward: 0.0673887062836. Std of Reward: 0.424940252248.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 476000. Mean Reward: 0.215989304813. Std of Reward: 0.821024180989.\n",
      "Brain : HunterBrain. Step: 476000. Mean Reward: 0.147927353104. Std of Reward: 0.47963297914.\n",
      "Brain : HunteeBrain. Step: 477000. Mean Reward: 0.241005586592. Std of Reward: 0.801504935718.\n",
      "Brain : HunterBrain. Step: 477000. Mean Reward: 0.123019294938. Std of Reward: 0.465642902898.\n",
      "Brain : HunteeBrain. Step: 478000. Mean Reward: 0.21131147541. Std of Reward: 0.811395227037.\n",
      "Brain : HunterBrain. Step: 478000. Mean Reward: 0.112317920224. Std of Reward: 0.463668127019.\n",
      "Brain : HunteeBrain. Step: 479000. Mean Reward: 0.25472826087. Std of Reward: 0.817593853185.\n",
      "Brain : HunterBrain. Step: 479000. Mean Reward: 0.0975443809119. Std of Reward: 0.452916071697.\n",
      "Brain : HunteeBrain. Step: 480000. Mean Reward: 0.19422459893. Std of Reward: 0.828243946623.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 480000. Mean Reward: 0.157525942117. Std of Reward: 0.489092720575.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 481000. Mean Reward: 0.234139784946. Std of Reward: 0.827962625662.\n",
      "Brain : HunterBrain. Step: 481000. Mean Reward: 0.151973528375. Std of Reward: 0.485433753352.\n",
      "Brain : HunteeBrain. Step: 482000. Mean Reward: 0.242131147541. Std of Reward: 0.819054729226.\n",
      "Brain : HunterBrain. Step: 482000. Mean Reward: 0.147380286695. Std of Reward: 0.481018246165.\n",
      "Brain : HunteeBrain. Step: 483000. Mean Reward: 0.222406417112. Std of Reward: 0.82798452901.\n",
      "Brain : HunterBrain. Step: 483000. Mean Reward: 0.122953567641. Std of Reward: 0.465373658551.\n",
      "Brain : HunteeBrain. Step: 484000. Mean Reward: 0.331860465116. Std of Reward: 0.78822734831.\n",
      "Brain : HunterBrain. Step: 484000. Mean Reward: 0.0987803576335. Std of Reward: 0.44874526251.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain : HunteeBrain. Step: 485000. Mean Reward: 0.251847826087. Std of Reward: 0.817524004897.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 485000. Mean Reward: 0.117762823046. Std of Reward: 0.46877182885.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 486000. Mean Reward: 0.196577540107. Std of Reward: 0.817359808666.\n",
      "Brain : HunterBrain. Step: 486000. Mean Reward: 0.138558606508. Std of Reward: 0.47525467229.\n",
      "Brain : HunteeBrain. Step: 487000. Mean Reward: 0.170263157895. Std of Reward: 0.830860324072.\n",
      "Brain : HunterBrain. Step: 487000. Mean Reward: 0.136927962586. Std of Reward: 0.475939920654.\n",
      "Brain : HunteeBrain. Step: 488000. Mean Reward: 0.0887128712871. Std of Reward: 0.843947189196.\n",
      "Brain : HunterBrain. Step: 488000. Mean Reward: 0.197277541843. Std of Reward: 0.504824574117.\n",
      "Brain : HunteeBrain. Step: 489000. Mean Reward: 0.190322580645. Std of Reward: 0.828462043975.\n",
      "Brain : HunterBrain. Step: 489000. Mean Reward: 0.145994898074. Std of Reward: 0.483076845901.\n",
      "Brain : HunteeBrain. Step: 490000. Mean Reward: 0.199368421053. Std of Reward: 0.818057790426.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 490000. Mean Reward: 0.118330915608. Std of Reward: 0.462286791907.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 491000. Mean Reward: 0.209411764706. Std of Reward: 0.821095639802.\n",
      "Brain : HunterBrain. Step: 491000. Mean Reward: 0.1421359809. Std of Reward: 0.479352539185.\n",
      "Brain : HunteeBrain. Step: 492000. Mean Reward: 0.164919786096. Std of Reward: 0.823792176289.\n",
      "Brain : HunterBrain. Step: 492000. Mean Reward: 0.141717845624. Std of Reward: 0.477514415904.\n",
      "Brain : HunteeBrain. Step: 493000. Mean Reward: 0.306333333333. Std of Reward: 0.808228446803.\n",
      "Brain : HunterBrain. Step: 493000. Mean Reward: 0.145091636096. Std of Reward: 0.473014942553.\n",
      "Brain : HunteeBrain. Step: 494000. Mean Reward: 0.270718232044. Std of Reward: 0.81908049318.\n",
      "Brain : HunterBrain. Step: 494000. Mean Reward: 0.116169416109. Std of Reward: 0.464858269653.\n",
      "Brain : HunteeBrain. Step: 495000. Mean Reward: 0.240165745856. Std of Reward: 0.80698228885.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 495000. Mean Reward: 0.118911988255. Std of Reward: 0.462881244281.\n",
      "Saved Model\n",
      "Brain : HunteeBrain. Step: 496000. Mean Reward: 0.0918719211823. Std of Reward: 0.839223452544.\n",
      "Brain : HunterBrain. Step: 496000. Mean Reward: 0.202104988989. Std of Reward: 0.509551160427.\n",
      "Brain : HunteeBrain. Step: 497000. Mean Reward: 0.195691489362. Std of Reward: 0.819819032011.\n",
      "Brain : HunterBrain. Step: 497000. Mean Reward: 0.119175870274. Std of Reward: 0.464888267001.\n",
      "Brain : HunteeBrain. Step: 498000. Mean Reward: 0.2075. Std of Reward: 0.819510226214.\n",
      "Brain : HunterBrain. Step: 498000. Mean Reward: 0.126980911545. Std of Reward: 0.470663151657.\n",
      "Brain : HunteeBrain. Step: 499000. Mean Reward: 0.261902173913. Std of Reward: 0.821687798532.\n",
      "Brain : HunterBrain. Step: 499000. Mean Reward: 0.12318942985. Std of Reward: 0.463216927865.\n",
      "Brain : HunteeBrain. Step: 500000. Mean Reward: 0.172164948454. Std of Reward: 0.84141477177.\n",
      "Saved Model\n",
      "Brain : HunterBrain. Step: 500000. Mean Reward: 0.179323807078. Std of Reward: 0.496726167752.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "action is not in graph",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4d450d888117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mexport_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/vincentpierre/Downloads/dev2/ml-agents/python/ppo_multi/models.pyc\u001b[0m in \u001b[0;36mexport_graph\u001b[0;34m(model_path, env_name, target_nodes)\u001b[0m\n\u001b[1;32m     54\u001b[0m                               \u001b[0moutput_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.bytes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                               \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_saver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                               restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\")\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincentpierre/tensorflow/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.pyc\u001b[0m in \u001b[0;36mfreeze_graph\u001b[0;34m(input_graph, input_saver, input_binary, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_blacklist)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0moutput_node_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         variable_names_blacklist=variable_names_blacklist)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincentpierre/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.pyc\u001b[0m in \u001b[0;36mconvert_variables_to_constants\u001b[0;34m(sess, input_graph_def, output_node_names, variable_names_whitelist, variable_names_blacklist)\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0;31m# This graph only includes the nodes needed to evaluate the output nodes, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;31m# removes unneeded nodes like those involved in saving and assignment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m   \u001b[0minference_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sub_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_node_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0mfound_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincentpierre/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.pyc\u001b[0m in \u001b[0;36mextract_sub_graph\u001b[0;34m(graph_def, dest_nodes)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdest_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_to_node_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s is not in graph\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0mnodes_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: action is not in graph"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# graphs = {}\n",
    "# for brain in env.external_brain_names:\n",
    "#     graphs[brain] =  tf.Graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "#curriculum is disabled\n",
    "# def get_progress(brain):\n",
    "#     if curriculum_file is not None:\n",
    "#         if env._curriculum.measure_type == \"progress\":\n",
    "#             return steps / max_steps\n",
    "#         elif env._curriculum.measure_type == \"reward\":\n",
    "#             return last_reward\n",
    "#         else:\n",
    "#             return None\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "models = {}\n",
    "\n",
    "for brain in env.external_brain_names:\n",
    "    with tf.variable_scope(re.sub('[^0-9a-zA-Z]+', '-', brain)):\n",
    "        models[brain] = create_agent_model(env.brains[brain], lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps)\n",
    "\n",
    "\n",
    "# is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "# use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "# use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_paths = {}\n",
    "for brain in env.external_brain_names:\n",
    "    summary_paths[brain] = './summaries/{}'.format(run_path+'_'+brain)\n",
    "    if not os.path.exists(summary_paths[brain]):\n",
    "        os.makedirs(summary_paths[brain])\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps = {}\n",
    "    last_rewards = {}\n",
    "    summary_writers = {}\n",
    "    for brain in env.external_brain_names:\n",
    "        steps[brain], last_rewards[brain] = sess.run([models[brain].global_step, models[brain].last_reward])    \n",
    "        summary_writers[brain] = tf.summary.FileWriter(summary_paths[brain])\n",
    "#         if train_model:\n",
    "#             trainers[brain].write_text(summary_writers[brain], 'Hyperparameters', hyperparameter_dict, steps)\n",
    "\n",
    "#     info = env.reset(train_mode=train_model, progress=get_progress())\n",
    "    info = env.reset(train_mode=train_model)\n",
    "    trainers = {}\n",
    "    for brain in env.external_brain_names:\n",
    "        trainers[brain] = Trainer(models[brain], sess, info[brain],\n",
    "           (env.brains[brain].action_space_type == \"continuous\"),\n",
    "            (env.brains[brain].number_observations > 0),\n",
    "             (env.brains[brain].state_space_size > 0),\n",
    "              train_model)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    while min([steps[b] for b in env.external_brain_names]) <= max_steps:\n",
    "        if env.global_done:\n",
    "#             info = env.reset(train_mode=train_model, progress=get_progress())\n",
    "            info = env.reset(train_mode=train_model)\n",
    "        # Decide and take an action\n",
    "        take_action_epsi = {}\n",
    "        take_action_actions = {}\n",
    "        take_action_a_dist = {}\n",
    "        take_action_value = {}\n",
    "        for brain in env.external_brain_names:\n",
    "            (take_action_epsi[brain], take_action_actions[brain], take_action_a_dist[brain], take_action_value[brain]\n",
    "                ) = trainers[brain].take_action(info[brain], env, brain, steps[brain])\n",
    "\n",
    "        new_info = env.step(take_action_actions)\n",
    "        for brain in env.external_brain_names:\n",
    "            trainers[brain].add_experiences(info[brain], new_info[brain], take_action_epsi[brain],\n",
    "                            take_action_actions[brain], take_action_a_dist[brain], take_action_value[brain])\n",
    "\n",
    "        info = new_info\n",
    "        for brain in env.external_brain_names:\n",
    "            trainers[brain].process_experiences(info[brain], time_horizon, gamma, lambd)\n",
    "            if len(trainers[brain].training_buffer['actions']) > buffer_size and train_model:\n",
    "                    # Perform gradient descent with experience buffer\n",
    "                    trainers[brain].update_model(batch_size, num_epoch)\n",
    "            if steps[brain] % summary_freq == 0 and steps[brain] != 0 and train_model:\n",
    "                # Write training statistics to tensorboard.\n",
    "                trainers[brain].write_summary(summary_writers[brain], brain, steps[brain], env._curriculum.lesson_number)\n",
    "            if steps[brain] % save_freq == 0 and steps[brain] != 0 and train_model:\n",
    "                # Save Tensorflow model\n",
    "                # This does not need to be for each brain \n",
    "                save_model(sess, model_path=model_path, steps=steps[brain], saver=saver)\n",
    "            steps[brain] += 1\n",
    "            sess.run(models[brain].increment_step)\n",
    "            if len(trainers[brain].stats['cumulative_reward']) > 0:\n",
    "                mean_reward = np.mean(trainers[brain].stats['cumulative_reward'])\n",
    "                sess.run(models[brain].update_reward, feed_dict={models[brain].new_reward: mean_reward})\n",
    "                last_reward = sess.run(models[brain].last_reward)\n",
    "    for brain in env.external_brain_names:\n",
    "        # Final save Tensorflow model\n",
    "        if steps[brain] != 0 and train_model:\n",
    "            save_model(sess, model_path=model_path, steps=steps[brain], saver=saver)\n",
    "env.close()\n",
    "nodes = []\n",
    "for brain in env.external_brain_names:\n",
    "    scope = (re.sub('[^0-9a-zA-Z]+', '-', brain)) + '/'\n",
    "    nodes +=[scope + x for x in [\"action\",\"value_estimate\",\"action_probs\"]]\n",
    "    \n",
    "export_graph(model_path, env_name, target_nodes=','.join(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 20 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 20 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 20 variables to const ops.\n",
      "134 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "nodes = []\n",
    "for brain in env.external_brain_names:\n",
    "    scope = (re.sub('[^0-9a-zA-Z]+', '-', brain)) + '/'\n",
    "    nodes +=[scope + x for x in [\"action\",\"value_estimate\",\"action_probs\"]]\n",
    "    \n",
    "export_graph(model_path, env_name, target_nodes=','.join(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
